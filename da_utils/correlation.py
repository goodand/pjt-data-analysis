# 'correlation.py'
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.feature_selection import mutual_info_regression

import datetime as dt

# ìƒê´€ê´€ê³„ ë¶„ì„
from scipy.stats import pearsonr, spearmanr, kendalltau

import sys
import os
sys.path.append('/Users/jaehyuntak/Desktop/pjt-data-analysis')

# í•œê¸€ í°íŠ¸ ì„¤ì •
from da_utils.font import setup_korean_font

# from scipy import stats # kendalltau, pearsonr, spearmanr ë“±ì€ scipy.statsì—ì„œ ì§ì ‘ ì„í¬íŠ¸
# from sklearn.feature_selection import mutual_info_regression




# ----

# da_utils/correlation.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Note: scipy.stats.kendalltauëŠ” analyze_kendall_tau í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë  ì˜ˆì •ì…ë‹ˆë‹¤.
# sklearn.feature_selection.mutual_info_regressionëŠ” analyze_mutual_information í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë  ì˜ˆì •ì…ë‹ˆë‹¤.
# í˜„ì¬ analyze_basic_correlations í•¨ìˆ˜ì—ì„œëŠ” ì´ë“¤ì„ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

def analyze_basic_correlations(df: pd.DataFrame, numeric_cols: list, strong_corr_threshold: float = 0.5):
    """
    ì£¼ì–´ì§„ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ì— ëŒ€í•´ í”¼ì–´ìŠ¨, ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê´€ê³„ ë° ê·¸ ì°¨ì´ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
    ì¶”ê°€ì ìœ¼ë¡œ ê°•í•œ ìƒê´€ê´€ê³„(ì ˆëŒ€ê°’ > strong_corr_threshold)ë§Œì„ í‘œì‹œí•œ íˆíŠ¸ë§µì„ ê·¸ë¦½ë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„.
        numeric_cols (list): ìƒê´€ê´€ê³„ ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸.
        strong_corr_threshold (float, optional): ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ íŒë‹¨í•˜ëŠ” ì„ê³„ê°’. ê¸°ë³¸ê°’ì€ 0.5.
    """
    print('=== ê¸°ë³¸ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹œì‘ ===')

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³ , ì—†ëŠ” ì»¬ëŸ¼ì€ ê²½ê³  í›„ ì œì™¸
    available_numeric_cols = [col for col in numeric_cols if col in df.columns]
    missing_cols = [col for col in numeric_cols if col not in df.columns]
    if missing_cols:
        print(f"ê²½ê³ : ë‹¤ìŒ ì»¬ëŸ¼ë“¤ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤: {missing_cols}. ì´ ì»¬ëŸ¼ë“¤ì€ ë¶„ì„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")
    
    if not available_numeric_cols:
        print("ì˜¤ë¥˜: ìƒê´€ê´€ê³„ ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    correlation_data = df[available_numeric_cols]

    # í”¼ì–´ìŠ¨ ìƒê´€ê´€ê³„ ê³„ì‚°
    pearson_corr = correlation_data.corr(method='pearson')
    # ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê´€ê³„ ê³„ì‚°
    spearman_corr = correlation_data.corr(method='spearman')

    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    a1, a2, a3, a4 = axes.ravel() # axes[0,0], axes[0,1], axes[1,0], axes[1,1] ëŒ€ì‹  ravel() ì‚¬ìš©

    # í”¼ì–´ìŠ¨ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    sns.heatmap(pearson_corr, annot=True, fmt='.2f', cmap='RdBu_r',
                center=0, square=True, ax=a1, cbar_kws={'label': 'Pearson ìƒê´€ê³„ìˆ˜'})
    a1.set_title('í”¼ì–´ìŠ¨ ìƒê´€ê´€ê³„ (ì„ í˜• ê´€ê³„)', fontsize=14)

    # ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    sns.heatmap(spearman_corr, annot=True, fmt='.2f', cmap='RdBu_r',
                center=0, square=True, ax=a2, cbar_kws={'label': 'Spearman ìƒê´€ê³„ìˆ˜'})
    a2.set_title('ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê´€ê³„ (ìˆœìœ„ ê¸°ë°˜)', fontsize=14)

    # í”¼ì–´ìŠ¨ vs ìŠ¤í”¼ì–´ë§Œ ì°¨ì´ íˆíŠ¸ë§µ (ë¹„ì„ í˜•ì„± ì§€í‘œ)
    corr_diff = abs(spearman_corr - pearson_corr)
    sns.heatmap(corr_diff, annot=True, fmt='.2f', cmap='Reds',
                square=True, ax=a3, cbar_kws={'label': '|ì°¨ì´|'})
    a3.set_title('í”¼ì–´ìŠ¨ vs ìŠ¤í”¼ì–´ë§Œ ì°¨ì´ (ë¹„ì„ í˜•ì„± ì§€í‘œ)', fontsize=14)

    # ê°•í•œ ìƒê´€ê´€ê³„ ë„¤íŠ¸ì›Œí¬
    strong_corr = pearson_corr.copy()
    strong_corr[abs(strong_corr) < strong_corr_threshold] = 0
    np.fill_diagonal(strong_corr.values, 0) # ìê¸° ìì‹ ê³¼ì˜ ìƒê´€ê´€ê³„ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬

    sns.heatmap(strong_corr, annot=True, fmt='.2f', cmap='RdBu_r',
                center=0, square=True, ax=a4, cbar_kws={'label': 'ê°•í•œ ìƒê´€ê´€ê³„'})
    a4.set_title(f'ê°•í•œ ìƒê´€ê´€ê³„ (|r| > {strong_corr_threshold})', fontsize=14)

    plt.tight_layout()
    plt.show()
    print('=== ê¸°ë³¸ ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ ===')
    
    # ê¸°ëŠ¥ ëª…ì„¸ì„œì— ë”°ë¼ DataFrameë“¤ì„ ë°˜í™˜
    return pearson_corr, spearman_corr, corr_diff, strong_corr



# analyze_basic_correlations í•¨ìˆ˜ ì½”ë“œ

def analyze_kendall_tau(df: pd.DataFrame, key_vars: list, p_value_threshold: float = 0.05, tau_threshold: float = 0.1) -> dict:
    """
    ì£¼ì–´ì§„ í•µì‹¬ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê³„ìˆ˜ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    ìœ ì˜ë¯¸í•œ (p_value < p_value_threshold) ê´€ê³„ ì¤‘ tau ì ˆëŒ€ê°’ì´ tau_threshold ì´ìƒì¸ ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„.
        key_vars (list): ì¼„ë‹¬ íƒ€ìš° ë¶„ì„ì— ì‚¬ìš©í•  ë³€ìˆ˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸.
        p_value_threshold (float, optional): p-value ìœ ì˜ë¯¸ì„± ì„ê³„ê°’. ê¸°ë³¸ê°’ì€ 0.05.
        tau_threshold (float, optional): tau ê°’ì˜ ì ˆëŒ€ê°’ ì„ê³„ê°’. ê¸°ë³¸ê°’ì€ 0.1.

    Returns:
        dict: ìœ ì˜ë¯¸í•œ ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê´€ê³„ ê²°ê³¼ (ì˜ˆ: {'var1 vs var2': {'tau': 0.X, 'p_value': 0.Y}}).
    """
    print('\n=== ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê³„ìˆ˜ ë¶„ì„ ì‹œì‘ ===')
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³ , ì—†ëŠ” ì»¬ëŸ¼ì€ ê²½ê³  í›„ ì œì™¸
    available_key_vars = [col for col in key_vars if col in df.columns]
    missing_cols = [col for col in key_vars if col not in df.columns]
    if missing_cols:
        print(f"ê²½ê³ : ë‹¤ìŒ í•µì‹¬ ë³€ìˆ˜ ì»¬ëŸ¼ë“¤ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤: {missing_cols}. ì´ ì»¬ëŸ¼ë“¤ì€ ë¶„ì„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")
    
    if len(available_key_vars) < 2:
        print("ì˜¤ë¥˜: ì¼„ë‹¬ íƒ€ìš° ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ë³€ìˆ˜ê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤.")
        return {}

    analysis_data = df[available_key_vars].copy()
    kendall_results = {}

    for idx, var1 in enumerate(available_key_vars):
        for var2 in available_key_vars[idx+1:]:
            tau, p_value = kendalltau(analysis_data[var1], analysis_data[var2])
            if p_value < p_value_threshold and abs(tau) > tau_threshold:
                kendall_results[f'{var1} vs {var2}'] = {'tau': tau, 'p_value': p_value}

    if not kendall_results:
        print(f"  ìœ ì˜ë¯¸í•œ ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê´€ê³„ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (p<{p_value_threshold}, |tau|>{tau_threshold}).")
    else:
        print("  ìœ ì˜ë¯¸í•œ ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê´€ê³„ ê²°ê³¼:")
        for rel, stats in kendall_results.items():
            print(f'    {rel} = {stats["tau"]:.3f} (p={stats["p_value"]:.3f})')
    
    print('=== ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê³„ìˆ˜ ë¶„ì„ ì™„ë£Œ ===')
    return kendall_results


# ... (analyze_basic_correlations, analyze_kendall_tau í•¨ìˆ˜ ì½”ë“œ) ...

def analyze_mutual_information(df: pd.DataFrame, target_col: str, key_vars: list = None, random_state: int = 42) -> pd.DataFrame:
    """
    ì£¼ì–´ì§„ íƒ€ê²Ÿ ì»¬ëŸ¼ì— ëŒ€í•œ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ ìƒí˜¸ ì •ë³´ëŸ‰(Mutual Information)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    ìƒí˜¸ ì •ë³´ëŸ‰ì€ ì„ í˜•/ë¹„ì„ í˜• ê´€ê³„ êµ¬ë¶„ ì—†ì´ ë³€ìˆ˜ ê°„ ì •ë³´ ê³µìœ  ì •ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„.
        target_col (str): ìƒí˜¸ ì •ë³´ëŸ‰ ê³„ì‚°ì˜ íƒ€ê²Ÿ ë³€ìˆ˜ ì´ë¦„.
        key_vars (list, optional): MI ë¶„ì„ì— ì‚¬ìš©í•  íŠ¹ì • ë³€ìˆ˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸. Noneì´ë©´ target_colì„ ì œì™¸í•œ ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‚¬ìš©.
        random_state (int, optional): ì¬í˜„ì„±ì„ ìœ„í•œ ë‚œìˆ˜ ì‹œë“œ. ê¸°ë³¸ê°’ì€ 42.

    Returns:
        pd.DataFrame: ê° Featureì™€ Target ê°„ì˜ MI_Scoreë¥¼ ë‹´ì€ DataFrame, MI_Score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬.
    """
    print('\n=== ìƒí˜¸ ì •ë³´ëŸ‰ ê¸°ë°˜ ì—°ê´€ì„± ë¶„ì„ ì‹œì‘ ===')
    
    if target_col not in df.columns:
        print(f"ì˜¤ë¥˜: íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_col}'ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['Feature', 'MI_Score'])

    analysis_data = df.copy()
    target = analysis_data[target_col]

    if key_vars:
        # key_varsê°€ ì œê³µë˜ë©´ í•´ë‹¹ ë³€ìˆ˜ë“¤ë§Œ ì‚¬ìš©
        available_features = [col for col in key_vars if col in analysis_data.columns and col != target_col]
        missing_features = [col for col in key_vars if col not in analysis_data.columns or col == target_col]
        if missing_features:
            print(f"ê²½ê³ : ë‹¤ìŒ Feature ì»¬ëŸ¼ë“¤ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ê±°ë‚˜ íƒ€ê²Ÿ ì»¬ëŸ¼ê³¼ ë™ì¼í•©ë‹ˆë‹¤: {missing_features}. ì´ ì»¬ëŸ¼ë“¤ì€ ë¶„ì„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")
        features = analysis_data[available_features]
    else:
        # key_varsê°€ ì—†ìœ¼ë©´ target_colì„ ì œì™¸í•œ ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‚¬ìš©
        numeric_cols = analysis_data.select_dtypes(include=np.number).columns.tolist()
        features = analysis_data[numeric_cols].drop(columns=[target_col], errors='ignore')

    if features.empty:
        print(f"ì˜¤ë¥˜: íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_col}'ì„ ì œì™¸í•œ ë¶„ì„í•  Featureê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['Feature', 'MI_Score'])

    mi_scores = mutual_info_regression(features, target, random_state=random_state)
    mi_results = pd.DataFrame({
        'Feature': features.columns,
        'MI_Score': mi_scores
    }).sort_values('MI_Score', ascending=False).reset_index(drop=True)

    print(f"  (íƒ€ê²Ÿ: {target_col} ê¸°ì¤€)")
    for _, row in mi_results.iterrows():
        print(f'  {row["Feature"]}: {row["MI_Score"]:.3f}')
    
    print('=== ìƒí˜¸ ì •ë³´ëŸ‰ ê¸°ë°˜ ì—°ê´€ì„± ë¶„ì„ ì™„ë£Œ ===')
    return mi_results


def detect_nonlinear_patterns(
    df: pd.DataFrame,
    u_shape_x_col: str,
    u_shape_y_col: str,
    saturation_x_col: str,
    saturation_y_col: str,
    exponential_x_col: str,
    exponential_y_col: str,
    scatter_x_col: str,
    scatter_y_col: str,
    high_value_segment_col: str,
    high_value_analysis_cols: list,
    q_segments: int = 10,
    quantile_threshold: float = 0.9
):
    """
    ë‹¤ì–‘í•œ ë¹„ì„ í˜• íŒ¨í„´(Uìí˜•, í¬í™”ì , ì§€ìˆ˜ì  ê´€ê³„ ë“±)ì„ íƒì§€í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
    ê° íŒ¨í„´ íƒì§€ì— í•„ìš”í•œ ì»¬ëŸ¼ëª…ì€ ì¸ìë¡œ ë°›ì•„ ë²”ìš©ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„.
        u_shape_x_col (str): Uìí˜• ê´€ê³„ íƒì§€ ì‹œ Xì¶•ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼.
        u_shape_y_col (str): Uìí˜• ê´€ê³„ íƒì§€ ì‹œ Yì¶•ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼.
        saturation_x_col (str): í¬í™”ì  íƒì§€ ì‹œ Xì¶•ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼.
        saturation_y_col (str): í¬í™”ì  íƒì§€ ì‹œ Yì¶•ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼.
        exponential_x_col (str): ì§€ìˆ˜ì  ê´€ê³„ íƒì§€ ì‹œ Xì¶•ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼.
        exponential_y_col (str): ì§€ìˆ˜ì  ê´€ê³„ íƒì§€ ì‹œ Yì¶•ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼.
        scatter_x_col (str): ì¼ë°˜ ì‚°ì ë„ Xì¶• ì»¬ëŸ¼.
        scatter_y_col (str): ì¼ë°˜ ì‚°ì ë„ Yì¶• ì»¬ëŸ¼.
        high_value_segment_col (str): íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸(ì˜ˆ: ìƒìœ„ 10%)ë¥¼ ì •ì˜í•  ê¸°ì¤€ ì»¬ëŸ¼.
        high_value_analysis_cols (list): ìƒìœ„ ì„¸ê·¸ë¨¼íŠ¸ì˜ íŠ¹ì„±ì„ ë¶„ì„í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸.
        q_segments (int, optional): Uìí˜•/í¬í™”ì  íƒì§€ ì‹œ ë°ì´í„°ë¥¼ ë‚˜ëˆŒ ë¶„ìœ„ìˆ˜. ê¸°ë³¸ê°’ì€ 10.
        quantile_threshold (float, optional): ìƒìœ„ ì„¸ê·¸ë¨¼íŠ¸ ê¸°ì¤€ ë¶„ìœ„ìˆ˜. ê¸°ë³¸ê°’ì€ 0.9 (ìƒìœ„ 10%).
    """
    print('\n=== ë¹„ì„ í˜• íŒ¨í„´ ì‹¬í™” íƒì§€ ì‹œì‘ ===')
    customer_stats_copy = df.copy() # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³€ê²½ ë°©ì§€ë¥¼ ìœ„í•´ ë³µì‚¬

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    a1, a2, a3, a4 = axes.ravel()

    # ëª¨ë“  í•„ìˆ˜ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ ì‚¬ì „ í™•ì¸
    required_cols = [
        u_shape_x_col, u_shape_y_col,
        saturation_x_col, saturation_y_col,
        exponential_x_col, exponential_y_col,
        scatter_x_col, scatter_y_col,
        high_value_segment_col
    ] + high_value_analysis_cols
    
    missing_cols_overall = [col for col in required_cols if col not in customer_stats_copy.columns]
    if missing_cols_overall:
        print(f"ì˜¤ë¥˜: ë‹¤ìŒ í•„ìˆ˜ ì»¬ëŸ¼ë“¤ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤: {missing_cols_overall}. ë¹„ì„ í˜• íŒ¨í„´ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        plt.close(fig) # ë¹ˆ í”Œë¡¯ ìƒì„± ë°©ì§€
        return

    # 1. Uìí˜• ê´€ê³„ íƒì§€ (ì˜ˆ: ê±´ì¶•ë…„ë„ vs ë‹¨ìœ„ë©´ì ë‹¹ê±°ë˜ê¸ˆì•¡)
    if u_shape_x_col in customer_stats_copy.columns and u_shape_y_col in customer_stats_copy.columns:
        try:
            customer_stats_copy[f'{u_shape_x_col}_Segment'] = pd.qcut(
                customer_stats_copy[u_shape_x_col], q=q_segments, labels=False, duplicates='drop'
            )
            u_shape_analysis = customer_stats_copy.groupby(f'{u_shape_x_col}_Segment').agg({
                u_shape_y_col: 'mean',
                u_shape_x_col: 'mean'
            }).round(2)
            a1.plot(u_shape_analysis[u_shape_x_col], u_shape_analysis[u_shape_y_col], marker='o', linewidth=2, markersize=8)
            a1.set_xlabel(f'{u_shape_x_col} í‰ê· ')
            a1.set_ylabel(f'{u_shape_y_col} í‰ê· ')
            a1.set_title(f'{u_shape_x_col} vs {u_shape_y_col} (Uìí˜• íƒì§€)', fontsize=14)
            a1.grid(True, alpha=0.3)
        except Exception as e:
            a1.set_title(f'{u_shape_x_col} vs {u_shape_y_col} (ë¶„ì„ ì˜¤ë¥˜)', fontsize=14)
            a1.text(0.5, 0.5, f'ì˜¤ë¥˜: {e}', horizontalalignment='center', verticalalignment='center', transform=a1.transAxes)
    else:
        a1.set_title(f'{u_shape_x_col} vs {u_shape_y_col} (ë°ì´í„° ë¶€ì¡±)', fontsize=14)
        a1.text(0.5, 0.5, 'í•„ìš” ì»¬ëŸ¼ ì—†ìŒ', horizontalalignment='center', verticalalignment='center', transform=a1.transAxes)


    # 2. í¬í™”ì  íƒì§€ (ì˜ˆ: ì „ìš©ë©´ì  vs ê±°ë˜ê¸ˆì•¡)
    if saturation_x_col in customer_stats_copy.columns and saturation_y_col in customer_stats_copy.columns:
        try:
            customer_stats_copy[f'{saturation_x_col}_Segment'] = pd.qcut(
                customer_stats_copy[saturation_x_col], q=q_segments, labels=False, duplicates='drop'
            )
            saturation_analysis = customer_stats_copy.groupby(f'{saturation_x_col}_Segment').agg({
                saturation_y_col: 'mean',
                saturation_x_col: 'mean'
            }).round(2)
            a2.plot(saturation_analysis[saturation_x_col], saturation_analysis[saturation_y_col], marker='s', linewidth=2, markersize=8)
            a2.set_xlabel(f'{saturation_x_col} í‰ê· ')
            a2.set_ylabel(f'{saturation_y_col} í‰ê· ')
            a2.set_title(f'{saturation_x_col} vs {saturation_y_col} (í¬í™”ì  íƒì§€)', fontsize=14)
            a2.grid(True, alpha=0.3)
        except Exception as e:
            a2.set_title(f'{saturation_x_col} vs {saturation_y_col} (ë¶„ì„ ì˜¤ë¥˜)', fontsize=14)
            a2.text(0.5, 0.5, f'ì˜¤ë¥˜: {e}', horizontalalignment='center', verticalalignment='center', transform=a2.transAxes)
    else:
        a2.set_title(f'{saturation_x_col} vs {saturation_y_col} (ë°ì´í„° ë¶€ì¡±)', fontsize=14)
        a2.text(0.5, 0.5, 'í•„ìš” ì»¬ëŸ¼ ì—†ìŒ', horizontalalignment='center', verticalalignment='center', transform=a2.transAxes)


    # 3. ì§€ìˆ˜ì  ê´€ê³„ í™•ì¸ (ì˜ˆ: ê±´ì¶•ê²½ê³¼ë…„ìˆ˜ vs ë‹¨ìœ„ë©´ì ë‹¹ê±°ë˜ê¸ˆì•¡)
    if exponential_x_col in customer_stats_copy.columns and exponential_y_col in customer_stats_copy.columns:
        try:
            # ìœ íš¨í•œ ë°ì´í„° í•„í„°ë§ (0 ë˜ëŠ” ìŒìˆ˜ ê°’ ì œì™¸)
            valid_data = customer_stats_copy[
                (customer_stats_copy[exponential_x_col] > 0) & 
                (customer_stats_copy[exponential_y_col] > 0)
            ].copy()
            
            if not valid_data.empty:
                a3.scatter(valid_data[exponential_x_col], valid_data[exponential_y_col], alpha=0.6, s=30)
                a3.set_xlabel(exponential_x_col)
                a3.set_ylabel(exponential_y_col)

                # ë¡œê·¸ ë³€í™˜ ì„ í˜•í™” ë° ìƒê´€ê³„ìˆ˜ ë¹„êµ
                log_x = np.log1p(valid_data[exponential_x_col])
                log_y = np.log1p(valid_data[exponential_y_col])
                original_corr = valid_data[exponential_x_col].corr(valid_data[exponential_y_col])
                log_corr = log_x.corr(log_y)
                a3.set_title(f'{exponential_x_col} vs {exponential_y_col}\nì›ë³¸r={original_corr:.2f}, ë¡œê·¸ë³€í™˜r={log_corr:.2f}', fontsize=14)
                a3.grid(True, alpha=0.3)
            else:
                a3.set_title(f'{exponential_x_col} vs {exponential_y_col} (ìœ íš¨ ë°ì´í„° ë¶€ì¡±)', fontsize=14)
                a3.text(0.5, 0.5, 'ìœ íš¨í•œ ì–‘ìˆ˜ ë°ì´í„° ì—†ìŒ', horizontalalignment='center', verticalalignment='center', transform=a3.transAxes)
        except Exception as e:
            a3.set_title(f'{exponential_x_col} vs {exponential_y_col} (ë¶„ì„ ì˜¤ë¥˜)', fontsize=14)
            a3.text(0.5, 0.5, f'ì˜¤ë¥˜: {e}', horizontalalignment='center', verticalalignment='center', transform=a3.transAxes)
    else:
        a3.set_title(f'{exponential_x_col} vs {exponential_y_col} (ë°ì´í„° ë¶€ì¡±)', fontsize=14)
        a3.text(0.5, 0.5, 'í•„ìš” ì»¬ëŸ¼ ì—†ìŒ', horizontalalignment='center', verticalalignment='center', transform=a3.transAxes)


    # 4. ì¼ë°˜ ì‚°ì ë„ ë° íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„± ë¶„ì„ (ì˜ˆ: ì¸µ vs ê±°ë˜ê¸ˆì•¡)
    if scatter_x_col in customer_stats_copy.columns and scatter_y_col in customer_stats_copy.columns:
        try:
            a4.scatter(customer_stats_copy[scatter_x_col], customer_stats_copy[scatter_y_col], alpha=0.6, s=30, color='purple')
            a4.set_xlabel(scatter_x_col)
            a4.set_ylabel(scatter_y_col)
            a4.set_title(f'{scatter_x_col} vs {scatter_y_col}', fontsize=14)
            a4.grid(True, alpha=0.3)

            # íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ (ì˜ˆ: ìƒìœ„ 10%) ê³ ê° íŠ¹ì„± ì¶œë ¥
            if high_value_segment_col in customer_stats_copy.columns and all(col in customer_stats_copy.columns for col in high_value_analysis_cols):
                high_segment = customer_stats_copy[high_value_segment_col] > customer_stats_copy[high_value_segment_col].quantile(quantile_threshold)
                print(f"\n  {high_value_segment_col} ìƒìœ„ {(1-quantile_threshold)*100:.0f}% ê³ ê° íŠ¹ì„±:")
                if not customer_stats_copy[high_segment].empty:
                    for col in high_value_analysis_cols:
                        if pd.api.types.is_numeric_dtype(customer_stats_copy[col]):
                            print(f"  - í‰ê·  {col}: {customer_stats_copy[high_segment][col].mean():,.1f}")
                        else:
                            # ë²”ì£¼í˜• ì»¬ëŸ¼ì˜ ê²½ìš° ìµœë¹ˆê°’ ë˜ëŠ” ìƒìœ„ Nê°œ ì¶œë ¥
                            top_values = customer_stats_copy[high_segment][col].value_counts(normalize=True).head(3)
                            if not top_values.empty:
                                print(f"  - {col} (ìƒìœ„): {top_values.index.tolist()} (ë¹„ìœ¨: {top_values.values.tolist()})")
                            else:
                                print(f"  - {col}: ë°ì´í„° ì—†ìŒ")
                else:
                    print("  - í•´ë‹¹ ì¡°ê±´ì˜ ê³ ê°ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"ê²½ê³ : {high_value_segment_col} ë˜ëŠ” {high_value_analysis_cols} ì¤‘ ì¼ë¶€ ì»¬ëŸ¼ì´ ì—†ì–´ íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            a4.set_title(f'{scatter_x_col} vs {scatter_y_col} (ë¶„ì„ ì˜¤ë¥˜)', fontsize=14)
            a4.text(0.5, 0.5, f'ì˜¤ë¥˜: {e}', horizontalalignment='center', verticalalignment='center', transform=a4.transAxes)
    else:
        a4.set_title(f'{scatter_x_col} vs {scatter_y_col} (ë°ì´í„° ë¶€ì¡±)', fontsize=14)
        a4.text(0.5, 0.5, 'í•„ìš” ì»¬ëŸ¼ ì—†ìŒ', horizontalalignment='center', verticalalignment='center', transform=a4.transAxes)

    plt.tight_layout()
    plt.show()
    print('=== ë¹„ì„ í˜• íŒ¨í„´ ì‹¬í™” íƒì§€ ì™„ë£Œ ===')


    # ì‹œê³„ì—´ ë¶„ì„



def analyze_time_series_trend(
    df: pd.DataFrame,
    date_col: str,
    value_col: str,
    resample_freq: str = 'M', # 'D', 'W', 'M', 'Q', 'Y' ë“±
    rolling_window: int = None
):
    """
    ì£¼ì–´ì§„ ë‚ ì§œ ì»¬ëŸ¼ê³¼ ìˆ˜ì¹˜í˜• ê°’ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ë°ì´í„°ì˜ ì¶”ì„¸ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    ë°ì´í„°ë¥¼ ì¼ë³„, ì£¼ë³„, ì›”ë³„, ë¶„ê¸°ë³„, ì—°ë„ë³„ ë“±ìœ¼ë¡œ ì§‘ê³„í•˜ê³ , ì´ë™ í‰ê· ì„ ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„.
        date_col (str): ë‚ ì§œ/ì‹œê°„ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ì»¬ëŸ¼ëª…. datetime íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        value_col (str): ì‹œê°„ íë¦„ì— ë”°ë¼ ë¶„ì„í•  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ëª….
        resample_freq (str, optional): ë°ì´í„°ë¥¼ ì§‘ê³„í•  ì‹œê°„ ê°„ê²©.
                                       'D' (ì¼ë³„), 'W' (ì£¼ë³„), 'M' (ì›”ë³„), 'Q' (ë¶„ê¸°ë³„), 'Y' (ì—°ë„ë³„) ë“±.
                                       ê¸°ë³¸ê°’ì€ 'M'.
        rolling_window (int, optional): ì´ë™ í‰ê· ì„ ê³„ì‚°í•  ìœˆë„ìš° í¬ê¸°.
                                        Noneì´ë©´ ì´ë™ í‰ê· ì„ ê·¸ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ None.
    """
    print(f'\n=== ì‹œê³„ì—´ ì¶”ì„¸ ë¶„ì„ ì‹œì‘ ({value_col} by {date_col}, {resample_freq} ë‹¨ìœ„) ===')

    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if date_col not in df.columns:
        print(f"ì˜¤ë¥˜: ë‚ ì§œ ì»¬ëŸ¼ '{date_col}'ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    if value_col not in df.columns:
        print(f"ì˜¤ë¥˜: ê°’ ì»¬ëŸ¼ '{value_col}'ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # ë‚ ì§œ ì»¬ëŸ¼ì´ datetime íƒ€ì…ì¸ì§€ í™•ì¸ ë° ë³€í™˜
    # .copy()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ df ë³€ê²½ ë°©ì§€
    temp_df = df.copy() 
    if not pd.api.types.is_datetime64_any_dtype(temp_df[date_col]):
        try:
            temp_df[date_col] = pd.to_datetime(temp_df[date_col])
            print(f"ì •ë³´: '{date_col}' ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì˜¤ë¥˜: '{date_col}' ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
            return
            
    # ê°’ ì»¬ëŸ¼ì´ ìˆ˜ì¹˜í˜•ì¸ì§€ í™•ì¸
    if not pd.api.types.is_numeric_dtype(temp_df[value_col]):
        print(f"ì˜¤ë¥˜: ê°’ ì»¬ëŸ¼ '{value_col}'ì´ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return

    # ë‚ ì§œ ì»¬ëŸ¼ì— NaNì´ ìˆëŠ” í–‰ ì œê±° (ì‹œê³„ì—´ ë¶„ì„ì˜ ì •í™•ì„±ì„ ìœ„í•´)
    analysis_df = temp_df[[date_col, value_col]].dropna(subset=[date_col]).set_index(date_col)
    
    if analysis_df.empty:
        print(f"ì˜¤ë¥˜: '{date_col}' ì»¬ëŸ¼ì— ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ì–´ ì‹œê³„ì—´ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì‹œê³„ì—´ ë°ì´í„° ì§‘ê³„
    resampled_data = analysis_df[value_col].resample(resample_freq).mean()

    # ì´ë™ í‰ê·  ê³„ì‚°
    if rolling_window is not None and rolling_window > 0:
        if len(resampled_data) < rolling_window:
            print(f"ê²½ê³ : ë¦¬ìƒ˜í”Œë§ëœ ë°ì´í„° í¬ì¸íŠ¸({len(resampled_data)}ê°œ)ê°€ ì´ë™ í‰ê·  ìœˆë„ìš°({rolling_window}ê°œ)ë³´ë‹¤ ì ìŠµë‹ˆë‹¤. ì´ë™ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            rolling_mean = None
        else:
            rolling_mean = resampled_data.rolling(window=rolling_window).mean()
    else:
        rolling_mean = None

    # ì‹œê°í™”
    plt.figure(figsize=(14, 7))
    plt.plot(resampled_data.index, resampled_data.values, label=f'{value_col} ({resample_freq} í‰ê· )', marker='o', markersize=4, linestyle='-')
    if rolling_mean is not None:
        plt.plot(rolling_mean.index, rolling_mean.values, label=f'{rolling_window} {resample_freq} ì´ë™ í‰ê· ', color='red', linestyle='--')

    plt.title(f'{value_col}ì˜ ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ì¶”ì„¸', fontsize=16)
    plt.xlabel('ë‚ ì§œ', fontsize=12)
    plt.ylabel(value_col, fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=10)
    plt.tight_layout()
    plt.show()
    
    print('=== ì‹œê³„ì—´ ì¶”ì„¸ ë¶„ì„ ì™„ë£Œ ===')





# ì›”ë³„/ì—°ë³„ ì„±ì¥ë¥ ì„ ê³„ì‚°í•˜ê³  ì‹œê°í™”
    

# ----

# da_utils/correlation.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats import kendalltau
from sklearn.feature_selection import mutual_info_regression

# analyze_basic_correlations í•¨ìˆ˜ëŠ” ì—¬ê¸°ì— ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# analyze_kendall_tau í•¨ìˆ˜ëŠ” ì—¬ê¸°ì— ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# analyze_mutual_information í•¨ìˆ˜ëŠ” ì—¬ê¸°ì— ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# detect_nonlinear_patterns í•¨ìˆ˜ëŠ” ì—¬ê¸°ì— ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# analyze_time_series_trend í•¨ìˆ˜ëŠ” ì—¬ê¸°ì— ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# ... (ì´ì „ í•¨ìˆ˜ ì½”ë“œë“¤) ...

def analyze_growth_rates(
    df: pd.DataFrame,
    date_col: str,
    value_col: str,
    group_by_cols: list = None,
    summary_window: int = 12,
    num_top_groups: int = None,    # New: ìƒìœ„ Nê°œ ê·¸ë£¹ë§Œ í”Œë¡œíŒ…
    num_bottom_groups: int = None  # New: í•˜ìœ„ Nê°œ ê·¸ë£¹ë§Œ í”Œë¡œíŒ…
):
    """
    ì£¼ì–´ì§„ ë‚ ì§œ ì»¬ëŸ¼ê³¼ ìˆ˜ì¹˜í˜• ê°’ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ì›”ë³„/ì—°ë³„ ì„±ì¥ë¥ (MoM, YoY)ì„ ê³„ì‚°í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
    ì´ë¥¼ í†µí•´ ë‹¨ê¸° ëª¨ë©˜í…€ê³¼ ê³„ì ˆì„±ì„ ë°°ì œí•œ ì¥ê¸° íŠ¸ë Œë“œë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ê·¸ë£¹í•‘ ì»¬ëŸ¼ì´ ì§€ì •ëœ ê²½ìš°, ìƒìœ„/í•˜ìœ„ Nê°œ ê·¸ë£¹ë§Œ ì„ íƒí•˜ì—¬ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ëª¨ë“  ì‹œê°í™”ëŠ” ê°œë³„ ê·¸ë¦¼(figure)ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„.
        date_col (str): ë‚ ì§œ/ì‹œê°„ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ì»¬ëŸ¼ëª…. datetime íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        value_col (str): ì„±ì¥ë¥ ì„ ê³„ì‚°í•  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ëª….
        group_by_cols (list, optional): value_colì„ ì§‘ê³„í•˜ê¸° ì „ì— ì¶”ê°€ì ìœ¼ë¡œ ê·¸ë£¹í•‘í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸.
                                        Noneì´ë©´ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì§‘ê³„í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ None.
        summary_window (int, optional): ìµœê·¼ ì„±ì¥ë¥  ìš”ì•½ì— ì‚¬ìš©í•  ê¸°ê°„(ì›”). ê¸°ë³¸ê°’ì€ 12.
        num_top_groups (int, optional): ê·¸ë£¹í•‘ ì‹œ, í‰ê·  value_col ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ Nê°œ ê·¸ë£¹ë§Œ í”Œë¡œíŒ….
                                        Noneì´ë©´ ìƒìœ„ ê·¸ë£¹ í•„í„°ë§ ì—†ìŒ.
        num_bottom_groups (int, optional): ê·¸ë£¹í•‘ ì‹œ, í‰ê·  value_col ê¸°ì¤€ìœ¼ë¡œ í•˜ìœ„ Nê°œ ê·¸ë£¹ë§Œ í”Œë¡œíŒ….
                                           Noneì´ë©´ í•˜ìœ„ ê·¸ë£¹ í•„í„°ë§ ì—†ìŒ.
    """
    print(f'\n=== ì„±ì¥ë¥  ë¶„ì„ ì‹œì‘ ({value_col} by {date_col}) ===')

    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    required_cols = [date_col, value_col]
    if group_by_cols:
        required_cols.extend(group_by_cols)
    
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"ì˜¤ë¥˜: ë‹¤ìŒ í•„ìˆ˜ ì»¬ëŸ¼ë“¤ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤: {missing_cols}. ì„±ì¥ë¥  ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë‚ ì§œ ì»¬ëŸ¼ì´ datetime íƒ€ì…ì¸ì§€ í™•ì¸ ë° ë³€í™˜
    temp_df = df.copy()
    if not pd.api.types.is_datetime64_any_dtype(temp_df[date_col]):
        try:
            temp_df[date_col] = pd.to_datetime(temp_df[date_col])
            print(f"ì •ë³´: '{date_col}' ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì˜¤ë¥˜: '{date_col}' ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
            return
            
    # ê°’ ì»¬ëŸ¼ì´ ìˆ˜ì¹˜í˜•ì¸ì§€ í™•ì¸
    if not pd.api.types.is_numeric_dtype(temp_df[value_col]):
        print(f"ì˜¤ë¥˜: ê°’ ì»¬ëŸ¼ '{value_col}'ì´ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return

    # ì›”ë³„ ì§‘ê³„ ë°ì´í„° ìƒì„±
    grouping_keys = []
    if group_by_cols:
        grouping_keys.extend(group_by_cols)
    grouping_keys.append(temp_df[date_col].dt.to_period('M'))

    monthly_data = temp_df.groupby(grouping_keys)[value_col].sum().reset_index()
    monthly_data.rename(columns={temp_df[date_col].dt.to_period('M').name: 'month'}, inplace=True)
    
    if not group_by_cols: # ê·¸ë£¹í•‘ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì»¬ëŸ¼ëª…ì„ 'value'ë¡œ ë³€ê²½
        monthly_data.columns = ['month', value_col] # value_col ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    
    monthly_data['month'] = monthly_data['month'].dt.to_timestamp() # Periodë¥¼ Timestampë¡œ ë³€í™˜

    # ê·¸ë£¹ë³„ ì„±ì¥ë¥  ê³„ì‚°
    if group_by_cols:
        growth_data_list = []
        for name, group in monthly_data.groupby(group_by_cols):
            group = group.sort_values('month').reset_index(drop=True)
            group[f'{value_col}_lag1'] = group[value_col].shift(1)
            group['mom_growth'] = (group[value_col] - group[f'{value_col}_lag1']) / group[f'{value_col}_lag1'] * 100
            
            group[f'{value_col}_lag12'] = group[value_col].shift(12)
            group['yoy_growth'] = (group[value_col] - group[f'{value_col}_lag12']) / group[f'{value_col}_lag12'] * 100
            growth_data_list.append(group)
        growth_data = pd.concat(growth_data_list).reset_index(drop=True)
    else:
        growth_data = monthly_data.sort_values('month').reset_index(drop=True)
        growth_data[f'{value_col}_lag1'] = growth_data[value_col].shift(1)
        growth_data['mom_growth'] = (growth_data[value_col] - growth_data[f'{value_col}_lag1']) / growth_data[f'{value_col}_lag1'] * 100
        
        growth_data[f'{value_col}_lag12'] = growth_data[value_col].shift(12)
        growth_data['yoy_growth'] = (growth_data[value_col] - growth_data[f'{value_col}_lag12']) / growth_data[f'{value_col}_lag12'] * 100

    # --- ê·¸ë£¹ í•„í„°ë§ ë¡œì§ (ìƒìœ„ Nê°œ, í•˜ìœ„ Nê°œ) ---
    growth_data_to_plot = growth_data.copy()
    selected_group_names = []

    if group_by_cols and (num_top_groups is not None or num_bottom_groups is not None):
        # ê·¸ë£¹ë³„ í‰ê·  value_col ê³„ì‚°í•˜ì—¬ ë­í‚¹
        group_avg_values = growth_data.groupby(group_by_cols)[value_col].mean().sort_values(ascending=False)
        
        if num_top_groups is not None and num_top_groups > 0:
            top_groups = group_avg_values.head(num_top_groups).index.tolist()
            selected_group_names.extend(top_groups)
            print(f"\n  ìƒìœ„ {num_top_groups}ê°œ ê·¸ë£¹: {top_groups}")
        
        if num_bottom_groups is not None and num_bottom_groups > 0:
            bottom_groups = group_avg_values.tail(num_bottom_groups).index.tolist()
            selected_group_names.extend(bottom_groups)
            print(f"  í•˜ìœ„ {num_bottom_groups}ê°œ ê·¸ë£¹: {bottom_groups}")
        
        # ì¤‘ë³µ ì œê±° (ìƒìœ„/í•˜ìœ„ ê·¸ë£¹ì´ ê²¹ì¹  ê²½ìš°)
        selected_group_names = list(set(selected_group_names))

        if selected_group_names:
            # group_by_colsê°€ í•˜ë‚˜ì¼ ê²½ìš°ì™€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš° ì²˜ë¦¬
            if len(group_by_cols) == 1:
                growth_data_to_plot = growth_data[growth_data[group_by_cols[0]].isin(selected_group_names)]
            else:
                # ì—¬ëŸ¬ ì»¬ëŸ¼ìœ¼ë¡œ ê·¸ë£¹í•‘ëœ ê²½ìš° íŠœí”Œ ë¹„êµ
                selected_group_names_tuples = [tuple(g) for g in selected_group_names]
                growth_data_to_plot = growth_data[
                    growth_data[group_by_cols].apply(tuple, axis=1).isin(selected_group_names_tuples)
                ]
            print(f"  ì´ {len(selected_group_names)}ê°œ ê·¸ë£¹ë§Œ ì‹œê°í™”í•©ë‹ˆë‹¤.")
        else:
            print("  ìƒìœ„/í•˜ìœ„ ê·¸ë£¹ í•„í„°ë§ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ê·¸ë£¹ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.")
            growth_data_to_plot = growth_data.copy()
    else:
        print("  ê·¸ë£¹ í•„í„°ë§ì´ ìš”ì²­ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë“  ê·¸ë£¹ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.")
        growth_data_to_plot = growth_data.copy()


    # ìµœê·¼ ì„±ì¥ë¥  ìš”ì•½ (í”Œë¡œíŒ…ë˜ëŠ” ê·¸ë£¹ì— ëŒ€í•´ì„œë§Œ)
    if not growth_data_to_plot.empty:
        if group_by_cols:
            print(f"\n  ìµœê·¼ {summary_window}ê°œì›” í‰ê·  ì„±ì¥ë¥  (ì‹œê°í™”ëœ ê·¸ë£¹ë³„):")
            for name, group in growth_data_to_plot.groupby(group_by_cols):
                avg_mom = group.tail(summary_window)['mom_growth'].mean()
                avg_yoy = group.tail(summary_window)['yoy_growth'].mean()
                group_name_str = ', '.join(map(str, name)) if isinstance(name, tuple) else str(name)
                print(f"  ê·¸ë£¹ ({group_name_str}): MoM={avg_mom:.1f}%, YoY={avg_yoy:.1f}%")
        else:
            avg_mom = growth_data_to_plot.tail(summary_window)['mom_growth'].mean()
            avg_yoy = growth_data_to_plot.tail(summary_window)['yoy_growth'].mean()
            print(f'\n  ìµœê·¼ {summary_window}ê°œì›” í‰ê·  MoM ì„±ì¥ë¥ : {avg_mom:.1f}%')
            print(f'  ìµœê·¼ {summary_window}ê°œì›” í‰ê·  YoY ì„±ì¥ë¥ : {avg_yoy:.1f}%')
    else:
        print("  ì„±ì¥ë¥ ì„ ê³„ì‚°í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    # --- ì‹œê°í™” (ê°œë³„ ê·¸ë¦¼ìœ¼ë¡œ ë¶„ë¦¬) ---
    if not growth_data_to_plot.empty:
        # 1. ì›”ë³„ ê°’ ì¶”ì´
        plt.figure(figsize=(12, 6))
        if group_by_cols:
            for name, group in growth_data_to_plot.groupby(group_by_cols):
                group_name_str = ', '.join(map(str, name)) if isinstance(name, tuple) else str(name)
                plt.plot(group['month'], group[value_col], marker='o', linewidth=2, label=f'ê·¸ë£¹ {group_name_str}')
            plt.legend()
        else:
            plt.plot(growth_data_to_plot['month'], growth_data_to_plot[value_col], marker='o', linewidth=2)
        plt.title(f'ì›”ë³„ {value_col} ì¶”ì´', fontsize=16)
        plt.ylabel(value_col, fontsize=12)
        plt.xlabel('ë‚ ì§œ', fontsize=12)
        plt.tick_params(axis='x', rotation=45)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        # 2. MoM ì„±ì¥ë¥ 
        plt.figure(figsize=(12, 6))
        if group_by_cols:
            for name, group in growth_data_to_plot.groupby(group_by_cols):
                group_name_str = ', '.join(map(str, name)) if isinstance(name, tuple) else str(name)
                plt.plot(group['month'], group['mom_growth'], marker='o', linewidth=1, label=f'ê·¸ë£¹ {group_name_str}')
            plt.legend()
        else:
            plt.bar(growth_data_to_plot['month'], growth_data_to_plot['mom_growth'], width=15)
        plt.axhline(y=0, color='red', linestyle='-', alpha=0.7)
        plt.title('ì›”ê°„ ì„±ì¥ë¥  (MoM)', fontsize=16)
        plt.ylabel('ì„±ì¥ë¥  (%)', fontsize=12)
        plt.xlabel('ë‚ ì§œ', fontsize=12)
        plt.tick_params(axis='x', rotation=45)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        # 3. YoY ì„±ì¥ë¥ 
        plt.figure(figsize=(12, 6))
        if group_by_cols:
            for name, group in growth_data_to_plot.groupby(group_by_cols):
                group_name_str = ', '.join(map(str, name)) if isinstance(name, tuple) else str(name)
                plt.plot(group['month'], group['yoy_growth'], marker='o', linewidth=1, label=f'ê·¸ë£¹ {group_name_str}')
            plt.legend()
        else:
            plt.bar(growth_data_to_plot['month'], growth_data_to_plot['yoy_growth'], color='lightgreen', width=15)
        plt.axhline(y=0, color='red', linestyle='-', alpha=0.7)
        plt.title('ì—°ê°„ ì„±ì¥ë¥  (YoY)', fontsize=16)
        plt.ylabel('ì„±ì¥ë¥  (%)', fontsize=12)
        plt.xlabel('ë‚ ì§œ', fontsize=12)
        plt.tick_params(axis='x', rotation=45)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        # 4. MoM ì„±ì¥ë¥  ë¶„í¬ (ê·¸ë£¹í•‘ì´ ì—†ëŠ” ê²½ìš°ë§Œ)
        if not group_by_cols:
            plt.figure(figsize=(12, 6))
            growth_rates_mom = growth_data_to_plot['mom_growth'].dropna()
            if not growth_rates_mom.empty:
                plt.hist(growth_rates_mom, bins=20, alpha=0.7, color='coral', edgecolor='black')
                plt.axvline(x=growth_rates_mom.mean(), color='red', linestyle='--', 
                                    label=f'í‰ê· : {growth_rates_mom.mean():.1f}%')
                plt.title('MoM ì„±ì¥ë¥  ë¶„í¬', fontsize=16)
                plt.xlabel('ì„±ì¥ë¥  (%)', fontsize=12)
                plt.ylabel('ë¹ˆë„', fontsize=12)
                plt.legend()
                plt.grid(True, alpha=0.3)
            else:
                plt.title('MoM ì„±ì¥ë¥  ë¶„í¬ (ë°ì´í„° ë¶€ì¡±)', fontsize=16)
                plt.text(0.5, 0.5, 'ì„±ì¥ë¥  ë°ì´í„° ì—†ìŒ', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)
            plt.tight_layout()
            plt.show()
        else:
            print("\n  MoM ì„±ì¥ë¥  ë¶„í¬ëŠ” ê·¸ë£¹ ë¶„ì„ ì‹œ ê°œë³„ ê·¸ë¦¼ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        print("  ì‹œê°í™”í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    
    print('=== ì„±ì¥ë¥  ë¶„ì„ ì™„ë£Œ ===')




#  ë‹¤ì–‘í•œ ê¸°ê°„ì˜ ì´ë™ í‰ê· ì„ ê³„ì‚°í•˜ê³  ì‹œê°í™”í•˜ë©°, ìµœê·¼ íŠ¸ë Œë“œ ë°©í–¥ì„±ì„ ë¶„ì„


def analyze_rolling_trends(
    df: pd.DataFrame,
    date_col: str,
    value_col: str,
    rolling_windows: list = None, # ì¼ ë‹¨ìœ„ ìœˆë„ìš° í¬ê¸° ë¦¬ìŠ¤íŠ¸
    trend_summary_windows: list = None # ìµœê·¼ íŠ¸ë Œë“œ ìš”ì•½ ê¸°ê°„ ë¦¬ìŠ¤íŠ¸
):
    """
    ì£¼ì–´ì§„ ë‚ ì§œ ì»¬ëŸ¼ê³¼ ìˆ˜ì¹˜í˜• ê°’ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ê¸°ê°„ì˜ ì´ë™ í‰ê· ì„ ê³„ì‚°í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
    ì´ë¥¼ í†µí•´ ë‹¨ê¸° ë³€ë™ì„±ì„ ì œê±°í•˜ê³  ì¥ê¸°ì ì¸ íŠ¸ë Œë“œ ë°©í–¥ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„.
        date_col (str): ë‚ ì§œ/ì‹œê°„ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ì»¬ëŸ¼ëª…. datetime íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        value_col (str): ì´ë™ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ëª….
        rolling_windows (list, optional): ê³„ì‚°í•  ì´ë™ í‰ê·  ìœˆë„ìš° í¬ê¸° ë¦¬ìŠ¤íŠ¸ (ì¼ ë‹¨ìœ„).
                                         ê¸°ë³¸ê°’ì€ [7, 30, 90] (ì£¼ê°„, ì›”ê°„, ë¶„ê¸°).
        trend_summary_windows (list, optional): ìµœê·¼ íŠ¸ë Œë“œ ë°©í–¥ì„±ì„ ìš”ì•½í•  ê¸°ê°„ ë¦¬ìŠ¤íŠ¸ (ì¼ ë‹¨ìœ„).
                                               ê¸°ë³¸ê°’ì€ [7, 30].
    """
    print(f'\n=== ì´ë™ í‰ê·  íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘ ({value_col} by {date_col}) ===')

    if rolling_windows is None:
        rolling_windows = [7, 30, 90] # ê¸°ë³¸ê°’ ì„¤ì •
    if trend_summary_windows is None:
        trend_summary_windows = [7, 30] # ê¸°ë³¸ê°’ ì„¤ì •

    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if date_col not in df.columns:
        print(f"ì˜¤ë¥˜: ë‚ ì§œ ì»¬ëŸ¼ '{date_col}'ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    if value_col not in df.columns:
        print(f"ì˜¤ë¥˜: ê°’ ì»¬ëŸ¼ '{value_col}'ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # ë‚ ì§œ ì»¬ëŸ¼ì´ datetime íƒ€ì…ì¸ì§€ í™•ì¸ ë° ë³€í™˜
    temp_df = df.copy()
    if not pd.api.types.is_datetime64_any_dtype(temp_df[date_col]):
        try:
            temp_df[date_col] = pd.to_datetime(temp_df[date_col])
            print(f"ì •ë³´: '{date_col}' ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì˜¤ë¥˜: '{date_col}' ì»¬ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
            return
            
    # ê°’ ì»¬ëŸ¼ì´ ìˆ˜ì¹˜í˜•ì¸ì§€ í™•ì¸
    if not pd.api.types.is_numeric_dtype(temp_df[value_col]):
        print(f"ì˜¤ë¥˜: ê°’ ì»¬ëŸ¼ '{value_col}'ì´ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return

    # ì¼ë³„ ê°’ ì§‘ê³„
    # ë‚ ì§œ ì»¬ëŸ¼ì— NaNì´ ìˆëŠ” í–‰ ì œê±° í›„ ì¼ë³„ í•©ê³„ (ë˜ëŠ” í‰ê· ) ê³„ì‚°
    daily_data = temp_df[[date_col, value_col]].dropna(subset=[date_col])
    daily_data['date_only'] = daily_data[date_col].dt.date # ë‚ ì§œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    daily_aggregated = daily_data.groupby('date_only')[value_col].sum().reset_index() # ì¼ë³„ í•©ê³„
    
    daily_aggregated.columns = ['date', value_col]
    daily_aggregated['date'] = pd.to_datetime(daily_aggregated['date'])
    daily_aggregated = daily_aggregated.sort_values('date').reset_index(drop=True)

    if daily_aggregated.empty:
        print(f"ì˜¤ë¥˜: '{date_col}' ì»¬ëŸ¼ì— ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ì–´ ì¼ë³„ ì§‘ê³„ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë‹¤ì–‘í•œ ê¸°ê°„ì˜ ì´ë™ í‰ê·  ê³„ì‚°
    for window in rolling_windows:
        if len(daily_aggregated) >= window:
            daily_aggregated[f'ma_{window}'] = daily_aggregated[value_col].rolling(window=window).mean()
        else:
            print(f"ê²½ê³ : ë°ì´í„° í¬ì¸íŠ¸({len(daily_aggregated)}ê°œ)ê°€ ì´ë™ í‰ê·  ìœˆë„ìš°({window}ì¼)ë³´ë‹¤ ì ì–´ ma_{window}ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # íŠ¸ë Œë“œ ë°©í–¥ì„± ë¶„ì„
    print("\nğŸ“ˆ íŠ¸ë Œë“œ ë°©í–¥ì„± ë¶„ì„ ê²°ê³¼:")
    for window in trend_summary_windows:
        ma_col = f'ma_{window}'
        if ma_col in daily_aggregated.columns:
            # ì´ë™ í‰ê· ì˜ ë³€í™”ëŸ‰ (diff) ê³„ì‚°
            daily_aggregated[f'trend_{window}'] = daily_aggregated[ma_col].diff()
            
            # ìµœê·¼ íŠ¸ë Œë“œ ìš”ì•½
            recent_trend_values = daily_aggregated[f'trend_{window}'].tail(window).dropna()
            if not recent_trend_values.empty:
                recent_trend_mean = recent_trend_values.mean()
                direction = 'ìƒìŠ¹' if recent_trend_mean > 0 else 'í•˜ë½' if recent_trend_mean < 0 else 'ìœ ì§€'
                print(f"  ìµœê·¼ {window}ì¼ ì´ë™í‰ê·  íŠ¸ë Œë“œ: {direction} ({recent_trend_mean:.2f})")
            else:
                print(f"  ìµœê·¼ {window}ì¼ íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        else:
            print(f"  {window}ì¼ ì´ë™í‰ê· ì´ ê³„ì‚°ë˜ì§€ ì•Šì•„ íŠ¸ë Œë“œ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    # íŠ¸ë Œë“œ ì‹œê°í™”
    plt.figure(figsize=(15, 8))

    plt.plot(daily_aggregated['date'], daily_aggregated[value_col], alpha=0.3, color='gray', label='ì¼ë³„ ê°’')
    
    colors = ['blue', 'red', 'green', 'purple', 'orange'] # ì´ë™í‰ê· ì„  ìƒ‰ìƒ
    for i, window in enumerate(rolling_windows):
        ma_col = f'ma_{window}'
        if ma_col in daily_aggregated.columns:
            plt.plot(daily_aggregated['date'], daily_aggregated[ma_col], 
                     color=colors[i % len(colors)], 
                     label=f'{window}ì¼ ì´ë™í‰ê· ', 
                     linewidth=2 if window == rolling_windows[0] else 1.5) # ì²« ë²ˆì§¸ MAë§Œ ë‘ê»ê²Œ

    plt.title(f'{value_col} íŠ¸ë Œë“œ ë¶„ì„ (ì´ë™í‰ê· )', fontsize=16)
    plt.xlabel('ë‚ ì§œ', fontsize=12)
    plt.ylabel(value_col, fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    print('=== ì´ë™ í‰ê·  íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ ===')



#  í†µí•© ì‹¤í–‰ í•¨ìˆ˜

def run_full_correlation_analysis(
    df: pd.DataFrame,
    basic_corr_params: dict,
    advanced_corr_params: dict,
    nonlinear_patterns_params: dict,
    time_series_trend_params: dict,
    growth_rates_params: dict,
    rolling_trends_params: dict
):
    """
    ê³ ê° ë°ì´í„°ì— ëŒ€í•œ í¬ê´„ì ì¸ ìƒê´€ê´€ê³„ ë° ë¹„ì„ í˜• íŒ¨í„´, ì‹œê³„ì—´ ì¶”ì„¸, ì„±ì¥ë¥ , ì´ë™ í‰ê·  ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    ê° ë¶„ì„ ëª¨ë“ˆì— í•„ìš”í•œ íŒŒë¼ë¯¸í„°ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬ë°›ì•„ ë²”ìš©ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„.
        basic_corr_params (dict): analyze_basic_correlations í•¨ìˆ˜ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬.
                                  í•„ìˆ˜ í‚¤: 'numeric_cols'.
        advanced_corr_params (dict): analyze_kendall_tau ë° analyze_mutual_information í•¨ìˆ˜ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬.
                                     í•„ìˆ˜ í‚¤: 'key_vars', 'mi_target_col'.
        nonlinear_patterns_params (dict): detect_nonlinear_patterns í•¨ìˆ˜ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬.
                                          í•„ìˆ˜ í‚¤: 'u_shape_x_col', 'u_shape_y_col', 'saturation_x_col', 'saturation_y_col',
                                          'exponential_x_col', 'exponential_y_col', 'scatter_x_col', 'scatter_y_col',
                                          'high_value_segment_col', 'high_value_analysis_cols'.
        time_series_trend_params (dict): analyze_time_series_trend í•¨ìˆ˜ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬.
                                         í•„ìˆ˜ í‚¤: 'date_col', 'value_col'.
        growth_rates_params (dict): analyze_growth_rates í•¨ìˆ˜ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬.
                                    í•„ìˆ˜ í‚¤: 'date_col', 'value_col'.
        rolling_trends_params (dict): analyze_rolling_trends í•¨ìˆ˜ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬.
                                      í•„ìˆ˜ í‚¤: 'date_col', 'value_col'.
    """
    print("=== ê³ ê° í–‰ë™ ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ì‹œì‘ ===")

    # 1. ê¸°ë³¸ ìƒê´€ê´€ê³„ ë¶„ì„ ë° ì‹œê°í™”
    print("\n--- [1] ê¸°ë³¸ ìƒê´€ê´€ê³„ ë¶„ì„ ---")
    analyze_basic_correlations(df=df, **basic_corr_params)

    # 2. ê³ ê¸‰ ìƒê´€ê´€ê³„ ë¶„ì„ (ì¼„ë‹¬ íƒ€ìš°, ìƒí˜¸ ì •ë³´ëŸ‰)
    print("\n--- [2] ê³ ê¸‰ ìƒê´€ê´€ê³„ ë¶„ì„ ---")
    # ì¼„ë‹¬ íƒ€ìš°
    kendall_results = analyze_kendall_tau(
        df=df,
        key_vars=advanced_corr_params['key_vars'],
        p_value_threshold=advanced_corr_params.get('kendall_p_value_threshold', 0.05),
        tau_threshold=advanced_corr_params.get('kendall_tau_threshold', 0.1)
    )
    # ìƒí˜¸ ì •ë³´ëŸ‰
    mi_results = analyze_mutual_information(
        df=df,
        target_col=advanced_corr_params['mi_target_col'],
        key_vars=advanced_corr_params['key_vars'], # MI ë¶„ì„ì—ë„ key_vars ì‚¬ìš©
        random_state=advanced_corr_params.get('mi_random_state', 42)
    )

    # 3. ë¹„ì„ í˜• íŒ¨í„´ íƒì§€ ë° ì‹œê°í™”
    print("\n--- [3] ë¹„ì„ í˜• íŒ¨í„´ íƒì§€ ---")
    detect_nonlinear_patterns(df=df, **nonlinear_patterns_params)

    # 4. ì‹œê³„ì—´ ì¶”ì„¸ ë¶„ì„
    print("\n--- [4] ì‹œê³„ì—´ ì¶”ì„¸ ë¶„ì„ ---")
    analyze_time_series_trend(df=df, **time_series_trend_params)

    # 5. ì„±ì¥ë¥  ë¶„ì„
    print("\n--- [5] ì„±ì¥ë¥  ë¶„ì„ ---")
    analyze_growth_rates(df=df, **growth_rates_params)
    
    # 6. ì´ë™ í‰ê·  íŠ¸ë Œë“œ ë¶„ì„
    print("\n--- [6] ì´ë™ í‰ê·  íŠ¸ë Œë“œ ë¶„ì„ ---")
    analyze_rolling_trends(df=df, **rolling_trends_params)

    print("\n=== ê³ ê° í–‰ë™ ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ì™„ë£Œ ===")