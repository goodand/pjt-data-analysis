# %% [markdown]
# # 'encoder_converter.py'
# - ì—¬ê¸° ìˆëŠ” ì½”ë“œë¥¼ ì‹¤í–‰í•´ì„œ ê²€ì¦ í›„ ëª¨ë“ˆí™” í•  ìˆ˜ ìˆì–´ì•¼ í•¨
# - ì²˜ìŒì— íŒŒì¼ ì„¤ì • ë“± ì˜ëª»ëœ ì½”ë“œê°€ ìˆì—ˆìŒ

# %% [markdown]
# 

# %%
import pandas as pd
import glob
import os

# CSV íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ
source_folder = "IsADirectoryError                         Traceback (most recent call last)
Cell In[14], line 7
      4 file_path = "/Users/jaehyuntak/Desktop/Project_____á„’á…§á†«á„Œá…¢ á„Œá…µá†«á„’á…¢á†¼á„Œá…®á†¼á„‹á…µá†«/pjt-data-Note/original_data"
      6 # CSV ë¶ˆëŸ¬ì˜¤ê¸°
----> 7 df = pd.read_csv(file_path, encoding='utf-8')  # ì¸ì½”ë”© utf-8, euc-kr ë“± íŒŒì¼ì— ë§ì¶° ì¡°ì •
      9 # ë°ì´í„° í™•ì¸
     10 print(df.head())       # ìƒìœ„ 5í–‰ í™•ì¸

File ~/Desktop/pjt-data-analysis/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)
   1013 kwds_defaults = _refine_defaults_read(
   1014     dialect,
   1015     delimiter,
   (...)   1022     dtype_backend=dtype_backend,
   1023 )
   1024 kwds.update(kwds_defaults)
-> 1026 return _read(filepath_or_buffer, kwds)

File ~/Desktop/pjt-data-analysis/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)
    617 _validate_names(kwds.get("names", None))
    619 # Create the parser.
--> 620 parser = TextFileReader(filepath_or_buffer, **kwds)
    622 if chunksize or iterator:
    623     return parser
...
--> 882         handle = open(handle, ioargs.mode)
    883     handles.append(handle)
    885 # Convert BytesIO or file objects passed with an encoding

IsADirectoryError: [Errno 21] Is a directory: '/Users/jaehyuntak/Desktop/Project_____á„’á…§á†«á„Œá…¢ á„Œá…µá†«á„’á…¢á†¼á„Œá…®á†¼á„‹á…µá†«/pjt-data-Note/original_data'"


# ëª¨ë“  CSV íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
csv_files = glob.glob(os.path.join(source_folder, "*.csv"))

# %%
import chardet
import pandas as pd

# ì¸ì½”ë”© ê°ì§€ í•¨ìˆ˜
def detect_encoding(file_path):
    """íŒŒì¼ì˜ ì¸ì½”ë”©ì„ ê°ì§€í•˜ëŠ” í•¨ìˆ˜"""
    try:
        with open(file_path, 'rb') as f:
            # íŒŒì¼ì˜ ì¼ë¶€ë§Œ ì½ì–´ì„œ ì¸ì½”ë”© ê°ì§€ (ì†ë„ í–¥ìƒ)
            raw_data = f.read(10000)  # ì²˜ìŒ 10KBë§Œ ì½ê¸°
            result = chardet.detect(raw_data)
            return result
    except Exception as e:
        return {'encoding': 'Error', 'confidence': 0, 'error': str(e)}

# ê° CSV íŒŒì¼ì˜ ì¸ì½”ë”© ê°ì§€
encoding_results = []

print(f"ì´ {len(csv_files)}ê°œì˜ CSV íŒŒì¼ ì¸ì½”ë”© ê°ì§€ ì¤‘...\n")

for i, file_path in enumerate(csv_files, 1):
    file_name = os.path.basename(file_path)
    print(f"[{i}/{len(csv_files)}] {file_name} ì²˜ë¦¬ ì¤‘...")
    
    # ì¸ì½”ë”© ê°ì§€
    result = detect_encoding(file_path)
    
    encoding_info = {
        'file_name': file_name,
        'file_path': file_path,
        'encoding': result.get('encoding', 'Unknown'),
        'confidence': result.get('confidence', 0),
        'file_size_mb': round(os.path.getsize(file_path) / (1024*1024), 2)
    }
    
    encoding_results.append(encoding_info)
    
    # ê²°ê³¼ ì¶œë ¥
    if result.get('encoding'):
        print(f"   â†’ ì¸ì½”ë”©: {result['encoding']} (ì‹ ë¢°ë„: {result['confidence']:.2%})")
    else:
        print(f"   â†’ ì˜¤ë¥˜: {result.get('error', 'ì¸ì½”ë”© ê°ì§€ ì‹¤íŒ¨')}")
    print()

# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬
encoding_df = pd.DataFrame(encoding_results)

# ê²°ê³¼ ìš”ì•½
print("=" * 60)
print("ì¸ì½”ë”© ê°ì§€ ê²°ê³¼ ìš”ì•½")
print("=" * 60)
print(encoding_df.to_string(index=False))

print(f"\nì¸ì½”ë”©ë³„ íŒŒì¼ ê°œìˆ˜:")
encoding_counts = encoding_df['encoding'].value_counts()
for encoding, count in encoding_counts.items():
    print(f"  {encoding}: {count}ê°œ")

# %%
def find_real_data_start(file_path, encoding='utf-8'):
    """
    ì‹¤ì œ ë°ì´í„° í…Œì´ë¸” ì‹œì‘ì ì„ ì°¾ëŠ” í•¨ìˆ˜
    ì—°ì†ëœ ì¤„ë“¤ì´ ê°™ì€ ì—´ ê°œìˆ˜ë¥¼ ê°€ì§€ê³  ì¶©ë¶„í•œ ì—´ì„ ê°€ì§„ ì§€ì ì„ ì°¾ê¸°
    """
    try:
        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
            lines = f.readlines()
        
        print(f"   ğŸ“„ ì „ì²´ ì¤„ ìˆ˜: {len(lines)}")
        
        # ê° ì¤„ì˜ ì—´ ê°œìˆ˜ ë¶„ì„
        line_info = []
        for i, line in enumerate(lines):
            clean_line = line.strip()
            if not clean_line:
                continue
            
            # ì‰¼í‘œë¡œ ë¶„ë¦¬
            parts = clean_line.split(',')
            # ë¹ˆ ë¶€ë¶„ ì œì™¸í•˜ê³  ì‹¤ì œ ë°ì´í„° ê°œìˆ˜ ê³„ì‚°
            meaningful_parts = [part.strip() for part in parts if part.strip()]
            col_count = len(meaningful_parts)
            
            line_info.append({
                'line_num': i,
                'col_count': col_count,
                'content': clean_line[:50] + '...' if len(clean_line) > 50 else clean_line
            })
        
        # ì—´ ê°œìˆ˜ë³„ ê·¸ë£¹í™”í•´ì„œ ë¶„ì„
        from collections import Counter
        col_count_freq = Counter([info['col_count'] for info in line_info])
        print(f"   ğŸ“Š ì—´ ê°œìˆ˜ ë¶„í¬: {dict(col_count_freq)}")
        
        # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” ì—´ ê°œìˆ˜ ì¤‘ 5ê°œ ì´ìƒì¸ ê²ƒ ì°¾ê¸°
        most_common_cols = [col for col, freq in col_count_freq.most_common() 
                           if col >= 5 and freq >= 2]  # ìµœì†Œ 5ì—´, ìµœì†Œ 2ë²ˆ ë‚˜íƒ€ë‚¨
        
        if not most_common_cols:
            print(f"   âŒ ì¶©ë¶„í•œ ì—´ì„ ê°€ì§„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
        
        target_col_count = most_common_cols[0]
        print(f"   ğŸ¯ ëª©í‘œ ì—´ ê°œìˆ˜: {target_col_count}")
        
        # ì—°ì†ìœ¼ë¡œ ëª©í‘œ ì—´ ê°œìˆ˜ë¥¼ ê°€ì§„ ì²« ë²ˆì§¸ ì§€ì  ì°¾ê¸°
        consecutive_count = 0
        data_start = None
        
        for info in line_info:
            if info['col_count'] == target_col_count:
                consecutive_count += 1
                if consecutive_count == 1:  # ì²« ë²ˆì§¸ ì¼ì¹˜í•˜ëŠ” ì¤„
                    data_start = info['line_num']
                if consecutive_count >= 2:  # 2ì¤„ ì—°ì† ì¼ì¹˜
                    print(f"   âœ… ë°ì´í„° ì‹œì‘ì : {data_start}ë²ˆì§¸ ì¤„")
                    return data_start
            else:
                consecutive_count = 0
                data_start = None
        
        # ì—°ì† 2ì¤„ì„ ëª» ì°¾ì•˜ë‹¤ë©´ ì²« ë²ˆì§¸ ì¼ì¹˜í•˜ëŠ” ì¤„ ì‚¬ìš©
        for info in line_info:
            if info['col_count'] == target_col_count:
                print(f"   âš ï¸ ë‹¨ì¼ ì¼ì¹˜ì  ì‚¬ìš©: {info['line_num']}ë²ˆì§¸ ì¤„")
                return info['line_num']
        
        return None
        
    except Exception as e:
        print(f"   âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return None

# %%
def clean_csv_with_auto_header(file_path):
    """
    í—¤ë” ìë™ ê°ì§€ ë°©ì‹: ë°ì´í„° ì‹œì‘ì  ë°”ë¡œ ìœ„ ì¤„ì„ í—¤ë”ë¡œ ê°€ì •
    """
    file_name = os.path.basename(file_path)
    print(f"\nğŸ“ {file_name}")
    
    encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']
    
    for encoding in encodings:
        try:
            # ë°ì´í„° ì‹œì‘ì  ì°¾ê¸° (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
            data_start_line = find_real_data_start(file_path, encoding)
            
            if data_start_line is None:
                continue
            
            # í—¤ë”ëŠ” ë°ì´í„° ì‹œì‘ì  ë°”ë¡œ ìœ„ ì¤„ë¡œ ê°€ì •
            header_line = data_start_line - 1
            skiprows = header_line if header_line > 0 else 0
            
            print(f"   ğŸ“‹ í—¤ë” ì¤„: {header_line}ë²ˆì§¸")
            print(f"   ğŸ“Š ë°ì´í„° ì‹œì‘: {data_start_line}ë²ˆì§¸")
            print(f"   ğŸ”„ skiprows: {skiprows}")
            
            # CSV ì½ê¸° (í—¤ë” í¬í•¨)
            df = pd.read_csv(file_path, 
                           encoding=encoding,
                           skiprows=skiprows,
                           on_bad_lines='skip')
            
            if df.empty:
                continue
            
            print(f"   âœ… ì½ê¸° ì„±ê³µ (ì¸ì½”ë”©: {encoding})")
            print(f"   ğŸ“Š Shape: {df.shape}")
            print(f"   ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")
            
            # ê¸°ë³¸ ì •ë¦¬
            df = df.dropna(how='all')
            df = df.drop_duplicates()
            df.columns = df.columns.astype(str).str.strip()
            
            print(f"   ğŸ§¹ ì •ë¦¬ í›„ shape: {df.shape}")
            
            return df, encoding, file_name
            
        except Exception as e:
            print(f"   âŒ {encoding} ì‹¤íŒ¨: {str(e)[:50]}...")
            continue
    
    print(f"   âŒ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
    return None, None, None

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if csv_files:
    test_df, test_encoding, test_name = clean_csv_with_auto_header(csv_files[0])
    
    if test_df is not None:
        print(f"\nğŸ‰ ìë™ í—¤ë” ê°ì§€ ì„±ê³µ!")
        print(f"ì»¬ëŸ¼: {list(test_df.columns)}")
        print(f"\nìƒ˜í”Œ ë°ì´í„°:")
        print(test_df.head(3))
    else:
        print(f"\nâŒ ìë™ í—¤ë” ê°ì§€ ì‹¤íŒ¨")

# %%
test_df.head(10)

# %%
def process_all_csv_files_final(csv_files):
    """
    ëª¨ë“  CSV íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í†µí•© ì¤€ë¹„
    """
    print("=" * 60)
    print("ì „ì²´ CSV íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
    print("=" * 60)
    
    processed_data = []
    
    for i, file_path in enumerate(csv_files, 1):
        print(f"\n[{i}/{len(csv_files)}] ì²˜ë¦¬ ì¤‘...")
        
        df, encoding, file_name = clean_csv_with_auto_header(file_path)
        
        if df is not None and not df.empty:
            # NO ì»¬ëŸ¼ ì‚­ì œ
            if 'NO' in df.columns:
                df = df.drop('NO', axis=1)
                print(f"   ğŸ—‘ï¸ NO ì»¬ëŸ¼ ì‚­ì œë¨")
            
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ì•ì˜ 8ìë¦¬)
            import re
            date_match = re.search(r'^(\d{8})', file_name)
            file_date = date_match.group(1) if date_match else '99999999'
            
            processed_data.append({
                'dataframe': df,
                'file_name': file_name,
                'file_date': file_date,
                'rows': len(df)
            })
            
            print(f"   âœ… ì™„ë£Œ: {len(df):,} í–‰, íŒŒì¼ë‚ ì§œ: {file_date}")
        else:
            print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨")
    
    print(f"\nğŸ‰ ì´ {len(processed_data)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
    return processed_data

# ì‹¤í–‰
processed_data = process_all_csv_files_final(csv_files)

# %%
def combine_csv_files_by_date(processed_data):
    """
    íŒŒì¼ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ CSV íŒŒì¼ë“¤ í†µí•©
    """
    print("\n" + "=" * 60)
    print("íŒŒì¼ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ ë° í†µí•©")
    print("=" * 60)
    
    if not processed_data:
        print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    # íŒŒì¼ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    processed_data.sort(key=lambda x: x['file_date'])
    
    print("ğŸ“… ì •ë ¬ëœ íŒŒì¼ ìˆœì„œ:")
    total_rows = 0
    for i, data in enumerate(processed_data, 1):
        date_formatted = f"{data['file_date'][:4]}-{data['file_date'][4:6]}-{data['file_date'][6:8]}"
        total_rows += data['rows']
        print(f"   {i}. {date_formatted} - {data['file_name'][:30]}... ({data['rows']:,} í–‰)")
    
    print(f"\nğŸ“Š ì´ ì˜ˆìƒ í–‰ ìˆ˜: {total_rows:,}")
    
    # DataFrameë“¤ í†µí•©
    print(f"\nğŸ”„ DataFrame í†µí•© ì¤‘...")
    dataframes = [data['dataframe'] for data in processed_data]
    combined_df = pd.concat(dataframes, ignore_index=True, sort=False)
    
    print(f"âœ… í†µí•© ì™„ë£Œ: {len(combined_df):,} í–‰, {len(combined_df.columns)} ì»¬ëŸ¼")
    
    # ìƒˆë¡œìš´ NO ì»¬ëŸ¼ ìƒì„± (1ë¶€í„° ì‹œì‘)
    combined_df.insert(0, 'NO', range(1, len(combined_df) + 1))
    print(f"ğŸ“‹ ìƒˆë¡œìš´ NO ì»¬ëŸ¼ ì¶”ê°€ë¨ (1~{len(combined_df)})")
    
    return combined_df

# ì‹¤í–‰
combined_df = combine_csv_files_by_date(processed_data)

# %%
from datetime import datetime

def save_combined_result(combined_df, output_folder):
    """
    í†µí•©ëœ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    """
    print("\n" + "=" * 60)
    print("ê²°ê³¼ ì €ì¥")
    print("=" * 60)
    
    if combined_df is None:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    # ì¶œë ¥ í´ë” ìƒì„±
    os.makedirs(output_folder, exist_ok=True)
    
    # íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d")
    output_filename = f"combined_ì•„íŒŒíŠ¸ë§¤ë§¤_{timestamp}.csv"
    output_path = os.path.join(output_folder, output_filename)
    
    # CSV ì €ì¥
    combined_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_filename}")
    print(f"ğŸ“ ê²½ë¡œ: {output_path}")
    print(f"ğŸ“Š ìµœì¢… í¬ê¸°: {len(combined_df):,} í–‰ Ã— {len(combined_df.columns)} ì»¬ëŸ¼")
    
    # ìš”ì•½ ì •ë³´
    print(f"\nğŸ“‹ ì»¬ëŸ¼ ì •ë³´:")
    for i, col in enumerate(combined_df.columns, 1):
        print(f"   {i:2d}. {col}")
    
    # ë¯¸ë¦¬ë³´ê¸°
    print(f"\nğŸ‘€ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 3í–‰):")
    print(combined_df.head(3))
    
    return output_path

# ì‹¤í–‰
output_folder = "/Users/jaehyuntak/Desktop/Project_____í˜„ì¬ ì§„í–‰ì¤‘ì¸/pjt-data-Note/combine_origin/Combined_Results"
result_path = save_combined_result(combined_df, output_folder)

if result_path:
    print(f"\nğŸ‰ í†µí•© ì‘ì—… ì™„ë£Œ!")
    print(f"ê²°ê³¼ íŒŒì¼: {os.path.basename(result_path)}")


